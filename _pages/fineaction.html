---
title: "FineAction Dataset"
permalink: /fineaction/
author_profile: false
---

			<!-- Main -->
					<p style="margin-top:3px; margin-bottom:12px">
						FineAction: A Fined Video Dataset for Temporal Action Localization<br>
						[<a href="https://arxiv.org/abs/2105.11107">paper</a>]
					</p>
					<p align="center" style="margin-top:12px">
							<a href="mailto:yi.liu1@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://www.yiliu.me/">Yi Liu</a>&nbsp&nbsp

							<a href="mailto:lmwang.nju@gmail.com"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="http://wanglimin.github.io">Limin Wang</a>&nbsp&nbsp

							<a href="mailto:xiao.ma@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href=" https://hypnosx.github.io/Atopos.github.io">Xiao Ma</a>&nbsp&nbsp

							<a href="mailto:yl.wang@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://scholar.google.com/citations?hl=zh-CN&user=hD948dkAAAAJ">Yali Wang</a>&nbsp&nbsp

							<a href="mailto:yu.qiao@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="http://mmlab.siat.ac.cn/yuqiao">Yu Qiao</a>&nbsp&nbsp
							

					</p>
					<p align="center">
						<a href="http://mmlab.siat.ac.cn">MMLAB @ Shenzhen Institute of Advanced Technology</a>
					</p>
					<p align="center">
						<a href="http://mcg.nju.edu.cn/en/index.html">MCG Group @ Nanjing University</a>
					</p>
					<div class="box">
						<center>
						<span class="image fit">
						<img src='/images/refinedaction.gif'>
						</span>
						</center>
						<!-- <span class="image fit">
						<img src="/images/fineaction/show1.png" alt="" />
						</span>
						<span class="image fit">
						<img src="/images/fineaction/show2.png" alt="" />
						</span> -->
						<!-- <p style="text-align:justify; text-justify:inter-ideograph;"><small>The 25fps tubelets of bounding boxes and fine-grained action category annotations in the sample frames of <i>MultiSports</i> dataset. Multiple concurrent action situations frequently appear in <i>MultiSports</i> with many starting and ending points in the long untrimmed video clips. The frames are cropped and sampled by stride 5 or 7 for visualization propose. Tubes with the same color represent the same person.</small></p> -->
						<hr />
						<header>
						<h3>Abstract</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">Temporal action localization (TAL) is an important and challenging problem in video understanding.
							 However, most existing TAL benchmarks are built upon the coarse granularity of action classes, which exhibits two major limitations in this task. 
							 First, coarse-level actions can make the localization models overfit in high-level context information, and ignore the atomic action details in the video. 
							 Second, the coarse action classes often lead to the ambiguous annotations of temporal boundaries, which are inappropriate for temporal action localization. 
							 To tackle these problems, we develop a novel large-scale and fine-grained video dataset, coined as <i><b>FineAction</i></b>, for temporal action localization. 
							 In total, FineAction contains 103K temporal instances of 106 action categories, annotated in 17K untrimmed videos. 
							 FineAction introduces new opportunities and challenges for temporal action localization, thanks to its distinct characteristics of fine action classes with rich diversity, dense annotations of multiple instances, and co-occurring actions of different classes. 
							 To benchmark FineAction, we systematically investigate the performance of several popular temporal localization methods on it, and deeply analyze the influence of short-duration and fine-grained instances in temporal action localization. 
							 We believe that FineAction can advance research of temporal action localization and beyond.</p>
						<!-- <header>
							<h3>Demo Video</h3>
							<p>Please choose "1080P" for better experience.</p>
						</header>
						<p align="center"><iframe width="600" height="320" src="https://www.youtube.com/embed/uGjvKYWZ5Ww" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p> -->
						<!-- <header>
						<h3>Hierarchy of Action Category</h3>
						</header>
						<span class="image fit"><img src="/images/ms_hier.png" alt=""/></span>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						The action vocabulary hierarchy and annotator interface of the <i>MultiSports</i> dataset. Our <i>MultiSports</i> has a two-level hierarchy of action vocabularies, where the actions of each sport are fine-grained.
						</p> -->
						<header>
						<h3>Dataset Statitics</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						Our <i><b>FineAction</i></b> is a large-scale dataset that is suitable for training deep learning models, which contains 103,324 instances for a total of 705 video hours. As a consequence, the number of instances in FineAction is as great as 6.17 per video and 975 per category. 
						</p>
						<ol>
							<li>
								Comparison with Related Benchmarks. Our FineAction is unique due to its fine-grained action classes, multi-label and dense annotations, relatively large-scale capacity, and rich action diversity. 
								<span class="image fit"><img src="/images/fineaction/tab1.png"  /></span>
							</li>
							<li>
								Number of instances per category. We plot the instance distribution of all the bottom-level categories in each top-level category. All the plots exhibit the natural long-tailed distribution.
								<span class="image fit"><img src="/images/fineaction/fig1.png"  /></span>
							</li>
						</ol>
						<!-- <header>
						<h3>Dataset Properties</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						Our <i>MultiSports</i> contains 66 fine-grained action categories from four different sports, selected from 247 competition records. The records are manually cut into 800 clips per sport to keep the balance of data size between sports, where we discard intervals with only background scenes, such as award, and select the highlights of competitions as video clips for action localization.
						</p>
						<ol>
							<li>
								Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries)
								<span class="image fit"><img src="/images/fineaction/tab1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries)
								<span class="image fit"><img src="/images/fineaction/pic1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Statistics of each action class's data size in <i>MultiSports</i> sorted by descending order with 4 colors indicating 4 different sports. For actions in the different sports sharing the same name, we add the name of sports after them. The natural long-tailed distribution of action categories raises new challenges for action localization models.
								<span class="image fit"><img src="/images/fineaction/pic2.png" alt="number of instances"/></span>
							</li>
						</ol> -->
						<header>
						<h3>Experiment Results</h3>
						</header>
						<ol>
							<li>
								Comparison of the state-of-the-art methods on the validation set of FineAction. Left: evaluation on action proposal generation, in terms of AR@AN. Right : evaluation on action detection, in terms of mAP at IoU thresholds from 0.5 to 0.95.
							<span class="image fit"><img src="/images/fineaction/exp1.png"   /></span>
							</li>
							<li>
								Error analysis on FineAction. (a)  Left : the error distribution over the number of predictions per video. G means the number of Ground-Truth instances. Right: the impact of error types, measured by the improvement gained from resolving a particular type of error. (b) Visualization of typical failure cases on FineAction. 
							<span class="image fit"><img src="/images/fineaction/exp2.png"   /></span>
							</li>
						</ol>
						<header>
						<h3>Download</h3>
						<p>Please refer to the <a href="https://competitions.codalab.org/competitions/32363">competition page</a> for more information.</p>
						
						<!-- <p >
						<a href="https://pan.baidu.com/s/1LiCXqqhsJAOf05oyOh_h6g">Raw_video</a> Access Code: e0e2 <br>
						</p>
						<p >
						<a href="https://pan.baidu.com/s/11DGhaS5Agtk9Ma_tSo9TaA">i3d feature</a> Access Code: bl7y <br>
						</p>
						<p >
						<a href="https://pan.baidu.com/s/15JONgemgCKa2Y8747wAVoA">i3d feature-100</a> Access Code: w0rx <br>
						</p> -->
						</header>
					</div>