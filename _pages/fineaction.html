---
title: "FineAction Dataset"
permalink: /fineaction/
author_profile: false
---

			<!-- Main -->
					<p style="margin-top:3px; margin-bottom:12px">
						FineAction: A Fined Video Dataset for Temporal Action Localization<br>
						[<a href="https://arxiv.org/abs/2105.11107">paper</a>]
					</p>
					<p align="center" style="margin-top:12px">
							<a href="mailto:yi.liu1@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://www.yiliu.me/">Yi Liu</a>&nbsp&nbsp

							<a href="mailto:lmwang.nju@gmail.com"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="http://wanglimin.github.io">Limin Wang</a>&nbsp&nbsp

							<a href="mailto:xiao.ma@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://scholar.google.com/citations?hl=zh-CN&user=hD948dkAAAAJ">Xiao Ma</a>&nbsp&nbsp

							<a href="yl.wang@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://scholar.google.com/citations?hl=zh-CN&user=hD948dkAAAAJ">Yali Wang</a>&nbsp&nbsp

							<a href="yu.qiao@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="http://mmlab.siat.ac.cn/yuqiao">Yu Qiao</a>&nbsp&nbsp
							

					</p>
					<p align="center">
						<a href="http://mmlab.siat.ac.cn">MMLAB @ Shenzhen Institute of Advanced Technology</a>
					</p>
					<p align="center">
						<a href="http://mcg.nju.edu.cn/en/index.html">MCG Group @ Nanjing University</a>
					</p>
					<div class="box">
						<center>
						<span class="image fit">
						<img src='/images/refinedaction.gif'>
						</span>
						</center>
						<!-- <span class="image fit">
						<img src="/images/fineaction/show1.png" alt="" />
						</span>
						<span class="image fit">
						<img src="/images/fineaction/show2.png" alt="" />
						</span> -->
						<!-- <p style="text-align:justify; text-justify:inter-ideograph;"><small>The 25fps tubelets of bounding boxes and fine-grained action category annotations in the sample frames of <i>MultiSports</i> dataset. Multiple concurrent action situations frequently appear in <i>MultiSports</i> with many starting and ending points in the long untrimmed video clips. The frames are cropped and sampled by stride 5 or 7 for visualization propose. Tubes with the same color represent the same person.</small></p> -->
						<hr />
						<header>
						<h3>Abstract</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">On the existing benchmark datasets, THUMOS14 and ActivityNet, temporal action localization techniques have achieved great success. However, there are still existing some problems, such as the source of the action is too single, there are only sports categories in THUMOS14,  coarse instances with uncertain boundaries in ActivityNet and HACS Segments interfering with proposal generation and behavior prediction.To take temporal action localization to a new level, we develop <i><b>FineAction</i></b>, a new large-scale fined video dataset collected from existing video datasets and web videos. Overall, this dataset contains 139K fined action instances densely annotated in almost 17K untrimmed videos spanning 106 action categories. FineAction has a more fined definition of action categories and high-quality annotations to reduce the boundary uncertainty compared to the existing action localization datasets. We systematically investigate representative methods of temporal action localization on our dataset and obtain some interesting findings with further analysis. Experimental results reveal that our FineAction brings new challenges for action localization on fined and multi-label instances with shorter duration. This dataset will be public in the future and we hope our FineAction could advance research towards temporal action localization.</p>
						<!-- <header>
							<h3>Demo Video</h3>
							<p>Please choose "1080P" for better experience.</p>
						</header>
						<p align="center"><iframe width="600" height="320" src="https://www.youtube.com/embed/uGjvKYWZ5Ww" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p> -->
						<!-- <header>
						<h3>Hierarchy of Action Category</h3>
						</header>
						<span class="image fit"><img src="/images/ms_hier.png" alt=""/></span>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						The action vocabulary hierarchy and annotator interface of the <i>MultiSports</i> dataset. Our <i>MultiSports</i> has a two-level hierarchy of action vocabularies, where the actions of each sport are fine-grained.
						</p> -->
						<header>
						<h3>Dataset Statitics</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						Our <i><b>FineAction</i></b> is a large-scale dataset that is suitable for training deep learning models, which contains 103,324 instances for a total of 705 video hours. As a consequence, the number of instances in FineAction is as great as 6.17 per video and 975 per category. 
						</p>
						<ol>
							<li>
								Comparing FineAction with existing datasets for temporal action localization.
								<span class="image fit"><img src="/images/fineaction/tab1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Number of instances per category for FineAction.
								<span class="image fit"><img src="/images/fineaction/pic1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Comparison of datasets: the number of instances  (a) and categories per video (c). The distribution of duration of instances (b) and overlap (d) on FineAction.
								<span class="image fit"><img src="/images/fineaction/pic2.png" alt="number of instances"/></span>
							</li>
						</ol>
						<!-- <header>
						<h3>Dataset Properties</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						Our <i>MultiSports</i> contains 66 fine-grained action categories from four different sports, selected from 247 competition records. The records are manually cut into 800 clips per sport to keep the balance of data size between sports, where we discard intervals with only background scenes, such as award, and select the highlights of competitions as video clips for action localization.
						</p>
						<ol>
							<li>
								Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries)
								<span class="image fit"><img src="/images/fineaction/tab1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries)
								<span class="image fit"><img src="/images/fineaction/pic1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Statistics of each action class's data size in <i>MultiSports</i> sorted by descending order with 4 colors indicating 4 different sports. For actions in the different sports sharing the same name, we add the name of sports after them. The natural long-tailed distribution of action categories raises new challenges for action localization models.
								<span class="image fit"><img src="/images/fineaction/pic2.png" alt="number of instances"/></span>
							</li>
						</ol> -->
						<header>
						<h3>Experiment Results</h3>
						</header>
						<ol>
							<li>
							Comparisons with state-of-the-art proposal generation methods on our FineAction in terms of AR@AN, where SNMS stands forSoft-NMS.
							<span class="image fit"><img src="/images/fineaction/exp2.png" alt="Comparison of SOTA methods" /></span>
							</li>
							<li>
							Performance evaluation of state-of-the-art methods on our FineAction in terms of mAP at IoU thresholds from 0.3 to 0.7.
							<span class="image fit"><img src="/images/fineaction/exp3.png" alt="Comparison between SlowFast and SlowOnly" /></span>
							</li>
						</ol>
						<header>
						<h3>Download</h3>
						<!-- <p>WILL BE UPDATED LATER</p> -->
						<p >
						<a href="https://pan.baidu.com/s/1LiCXqqhsJAOf05oyOh_h6g">Raw_video</a> Access Code: e0e2 <br>
						</p>
						<p >
						<a href="https://pan.baidu.com/s/11DGhaS5Agtk9Ma_tSo9TaA">i3d feature</a> Access Code: bl7y <br>
						</p>
						<p >
						<a href="https://pan.baidu.com/s/15JONgemgCKa2Y8747wAVoA">i3d feature-100</a> Access Code: w0rx <br>
						</p>
						</header>
					</div>