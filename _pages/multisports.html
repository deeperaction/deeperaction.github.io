---
title: "MultiSports Dataset"
permalink: /multisports/
author_profile: false
---

			<!-- Main -->
					<p style="margin-top:3px; margin-bottom:12px">
						A Multi-Person Video Dataset of Spatio-Temporally Localized Sports Actions<br>
						[<a href="http://arxiv.org/abs/2105.07404">paper</a>] [<a href="https://github.com/MCG-NJU/MultiSports/">github</a>]
					</p>
					<p align="center" style="margin-top:12px">
							<a href="mailto:liyixxxuan@gmail.com"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://yixuanli98.github.io/">Yixuan Li</a>&nbsp&nbsp

							<a href="mailto:cl_chenlei@smail.nju.edu.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://github.com/MiaSanLei">Lei Chen</a>&nbsp&nbsp

							<a href="mailto:runyu.he@smail.nju.edu.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://judie1999.github.io">Runyu He</a>&nbsp&nbsp

							<a href="mailto:zhenzhiwang@outlook.com"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://github.com/zhenzhiwang">Zhenzhi Wang</a><br>

							<a href="mailto:gswu@nju.edu.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="http://mcg.nju.edu.cn/member/gswu/en/index.html">Gangshan Wu</a>&nbsp&nbsp
							
							<a href="mailto:lmwang.nju@gmail.com"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="http://wanglimin.github.io">Limin Wang</a>&nbsp&nbsp
					</p>
					<p align="center"><a href="http://mcg.nju.edu.cn/en/index.html">MCG Group @ Nanjing University</a></p>
					<p align="center">International Conference on Computer Vision (<a href="http://iccv2021.thecvf.com">ICCV</a>) 2021, Poster Presentation</p>
					<div class="box">
						<span class="image fit">
						<img src="/images/ms_visual_min.png" alt="" />
						</span>
						<p style="text-align:justify; text-justify:inter-ideograph;"><small>The 25fps tubelets of bounding boxes and fine-grained action category annotations in the sample frames of <i>MultiSports</i> dataset. Multiple concurrent action situations frequently appear in <i>MultiSports</i> with many starting and ending points in the long untrimmed video clips. The frames are cropped and sampled by stride 5 or 7 for visualization propose. Tubes with the same color represent the same person.</small></p>
						<hr />
						<header>
						<h3>Download</h3>
						<p style="text-align:justify; text-justify:inter-ideograph;">Please register on our <a href="https://competitions.codalab.org/competitions/32066">competition page</a> to get data.</br>
						Participants of this track need to register on <a href="https://competitions.codalab.org/competitions/33355">this website</a> and submit predictions for evaluation.</p>
						<p style="text-align:justify; text-justify:inter-ideograph;"><i><u>Aerobic gymnastics</u>, <u>basketball</u> and <u>volleyball</u> data have player id annotations. Player id annotations will not affect this workshop and will be released later.</i></p>
						</header>
						<header>
						<h3>Abstract</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">Spatio-temporal action localization is an important and challenging problem in video understanding. The existing action detection benchmarks are limited in aspects of small numbers of instances in a trimmed video or low-level atomic actions. This paper aims to present a new multi-person dataset of spatio-temporal localized sports actions, coined as <i><b>MultiSports</b></i>. We first analyze the important ingredients of constructing a realistic and challenging dataset for spatio-temporal action localization by proposing three criteria: (1) motion dependent identification, (2) with well-defined boundaries, (3) high-level classes with relative complexity. Based on these guidelines, we build the dataset of MultiSports v1.0 by selecting 4 sports classes, collecting around 3200 video clips, and annotating around 37790 action instances with 907k bounding boxes. Our datasets are characterized with important properties of high diversity, detailed annotation, and high quality. Our MultiSports, with its realistic setting and dense annotations, exposes the intrinsic challenge of action localization. To benchmark this, we adapt several representative methods to our dataset and give an in-depth analysis on the difficulty of action localization in our dataset. We hope our MultiSports can serve as a standard benchmark for spatio-temporal action localization in the future.</p>
						<header>
							<h3>Demo Video</h3>
							<p>Please choose "1080P" for better experience. [<a href="https://www.youtube.com/embed/uGjvKYWZ5Ww">link</a>]</p>
						</header>
						<p align="center"><iframe width="600" height="320" src="https://www.youtube.com/embed/uGjvKYWZ5Ww" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe></p>
						<header>
						<h3>Hierarchy of Action Category</h3>
						</header>
						<span class="image fit"><img src="/images/ms_hier.png" alt=""/></span>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						The action vocabulary hierarchy and annotator interface of the <i>MultiSports</i> dataset. Our <i>MultiSports</i> has a two-level hierarchy of action vocabularies, where the actions of each sport are fine-grained.
						</p>
						<header>
						<h3>Dataset Statitics</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						Our <i>MultiSports</i> contains 66 fine-grained action categories from four different sports, selected from 247 competition records. The records are manually cut into 800 clips per sport to keep the balance of data size between sports, where we discard intervals with only background scenes, such as award, and select the highlights of competitions as video clips for action localization.
						</p>
						<ol>
							<li>
								Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries)
								<span class="image fit"><img src="/images/ms_comp.jpg" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Statistics of each action class's data size in <i>MultiSports</i> sorted by descending order with 4 colors indicating 4 different sports. For actions in the different sports sharing the same name, we add the name of sports after them. The natural long-tailed distribution of action categories raises new challenges for action localization models.
								<span class="image fit"><img src="/images/ms_num_inst.png" alt="number of instances"/></span>
							</li>
							<li>
								Statistics of action instance duration in <i>MultiSports</i>, where the x-axis is the number of frames and we count all instances longer than 95 frames in the last bar. Our action instances have a large variance in duration, resulting in challenges in modeling varying temporal structures.
								<span class="image fit"><img src="/images/ms_interval.png" alt="action duration"/></span>
							</li>
						</ol>
						<header>
						<h3>Experiment Results</h3>
						</header>
						<ol>
							<li>
							Comparison of SOTA methods
							<span class="image fit"><img src="/images/ms_sota.png" alt="Comparison of SOTA methods" /></span>
							</li>
							<li>
							Comparison between SlowFast and SlowOnly
							<span class="image fit"><img src="/images/ms_slowonly_slowfast_AP.png" alt="Comparison between SlowFast and SlowOnly" /></span>
							</li>
						</ol>
					</div>