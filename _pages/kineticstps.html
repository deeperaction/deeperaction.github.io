---
title: "Kinetics-TPS Dataset"
permalink: /kineticstps/
author_profile: false
---

			<!-- Main -->
					<p style="margin-top:3px; margin-bottom:12px">
						FineAction: A Fined Video Dataset for Temporal Action Localization<br>
						[<a href="https://arxiv.org/abs/2105.11107">paper</a>]
					</p>
					<p align="center" style="margin-top:12px">

							<a href="mailto:xiao.ma@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://scholar.google.com/citations?hl=zh-CN&user=hD948dkAAAAJ">Xiao Ma</a>&nbsp&nbsp

							<a href="mailto:xiao.ma@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://scholar.google.com/citations?hl=zh-CN&user=hD948dkAAAAJ">Ding Xia</a>&nbsp&nbsp

							<a href="yl.wang@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="https://scholar.google.com/citations?hl=zh-CN&user=hD948dkAAAAJ">Yali Wang</a>&nbsp&nbsp

							<a href="yu.qiao@siat.ac.cn"><i class="fas fa-envelope-open-text"></i></a>&nbsp
							<a href="http://mmlab.siat.ac.cn/yuqiao">Yu Qiao</a>&nbsp&nbsp
							

					</p>
					<p align="center">
						<a href="http://mmlab.siat.ac.cn">MMLAB @ Shenzhen Institute of Advanced Technology</a>
					</p>
					<div class="box">
						<center>
						<span class="image fit">
						<img src='/images/kineticstps.gif'>
						</span>
						</center>
						<!-- <span class="image fit">
						<img src="/images/fineaction/show1.png" alt="" />
						</span>
						<span class="image fit">
						<img src="/images/fineaction/show2.png" alt="" />
						</span> -->
						<!-- <p style="text-align:justify; text-justify:inter-ideograph;"><small>The 25fps tubelets of bounding boxes and fine-grained action category annotations in the sample frames of <i>MultiSports</i> dataset. Multiple concurrent action situations frequently appear in <i>MultiSports</i> with many starting and ending points in the long untrimmed video clips. The frames are cropped and sampled by stride 5 or 7 for visualization propose. Tubes with the same color represent the same person.</small></p> -->
						<hr />
						<header>
						<h3>Abstract</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">Traditionally, action recognition has been treated as a high-level video classification problem. However, such manner ignores detailed and middle-level understanding about human actions. To fill this gap, we deeply investigate explainable action recognition in videos, by explicitly encoding human actions as spatio-temporal composition of body parts and objects. Specifically, we first develop a large-scale <b><i>Kinetics-TPS</i></b> benchmark for this study. Different from existing video action datasets, our Kinetics-TPS provides 7.9M annotations of 10 body parts, 4.5M part gestures and 0.5 interactive objects in the frame level, which bring new opportunity to understand human action by compositional learning of body parts in 4743 videos. With Kinetics-TPS, we build up a novel progressive action graph network to recognize human actions in an explainable bottom-up manner, which progressively assembles body parts, their gestures and relevant objects as compositional human representation, and subsequently exploit spatio-temporal relations among humans for action recognition. Finally, we validate generalization capacity of our approaches in two extra challenging problems of compositional action recognition, i.e., semi-supervised and few-shot settings. Extensive results show that, with Kinetics-TPS, we can significantly boost action recognition performance with explainable body composition.</p>
						<!-- <header>
							<h3>Demo Video</h3>
							<p>Please choose "1080P" for better experience.</p>
						</header>
						<p align="center"><iframe width="600" height="320" src="https://www.youtube.com/embed/uGjvKYWZ5Ww" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p> -->
						<!-- <header>
						<h3>Hierarchy of Action Category</h3>
						</header>
						<span class="image fit"><img src="/images/ms_hier.png" alt=""/></span>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						The action vocabulary hierarchy and annotator interface of the <i>MultiSports</i> dataset. Our <i>MultiSports</i> has a two-level hierarchy of action vocabularies, where the actions of each sport are fine-grained.
						</p> -->
						<header>
						<h3>Dataset Statitics</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						Our <i><b>FineAction</i></b> is a large-scale dataset which provides 7.9M annotations of 10 body parts, 4.5M part gestures and 0.5 interactive objects in the frame level, which bring new opportunity to understand human action by compositional learning of body parts in 4743 videos.
						</p>
						<ol>
							<li>
								 Number of 'humans' instances per category of Kinetics-KPS.
								<span class="image fit"><img src="/images/kineticstps/1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								 Number of 'parts' instances per category of Kinetics-KPS.
								<span class="image fit"><img src="/images/kineticstps/2.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								 Number of  'objects' instances per category of Kinetics-KPS.
								<span class="image fit"><img src="/images/kineticstps/3.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								  Number of 'gestures'  instances per parts of Kinetics-KPS.
								<span class="image fit"><img src="/images/kineticstps/4.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								  Some 'parts-gestures' pairs instances of some categories in Kinetics-KPS.
								<span class="image fit"><img src="/images/kineticstps/5.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								  Top-5 frequent use of  'gestures' instances of parts in Kinetics-KPS.
								<span class="image fit"><img src="/images/kineticstps/6.png" alt="comparison with other dataset"/></span>
							</li>
						</ol>
						<!-- <header>
						<h3>Dataset Properties</h3>
						</header>
						<p style="text-align:justify; text-justify:inter-ideograph;">
						Our <i>MultiSports</i> contains 66 fine-grained action categories from four different sports, selected from 247 competition records. The records are manually cut into 800 clips per sport to keep the balance of data size between sports, where we discard intervals with only background scenes, such as award, and select the highlights of competitions as video clips for action localization.
						</p>
						<ol>
							<li>
								Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries)
								<span class="image fit"><img src="/images/fineaction/tab1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Overall comparison of statistics between existing action localization datasets and our <i>MultiSports</i> v1.0. (* only train and val sets' ground-truths are available, † number of person tracklets, each of which has one or more action labels, ‡ 1fps action annotations have no clear action boundaries)
								<span class="image fit"><img src="/images/fineaction/pic1.png" alt="comparison with other dataset"/></span>
							</li>
							<li>
								Statistics of each action class's data size in <i>MultiSports</i> sorted by descending order with 4 colors indicating 4 different sports. For actions in the different sports sharing the same name, we add the name of sports after them. The natural long-tailed distribution of action categories raises new challenges for action localization models.
								<span class="image fit"><img src="/images/fineaction/pic2.png" alt="number of instances"/></span>
							</li>
						</ol> -->
						<header>
						<h3>Experiment Results</h3>
						</header>
						<ol>
							<li>
							Comparisons with state-of-the-art proposal generation methods on our FineAction in terms of AR@AN, where SNMS stands forSoft-NMS.
							<span class="image fit"><img src="/images/fineaction/exp2.png" alt="Comparison of SOTA methods" /></span>
							</li>
							<li>
							Performance evaluation of state-of-the-art methods on our FineAction in terms of mAP at IoU thresholds from 0.3 to 0.7.
							<span class="image fit"><img src="/images/fineaction/exp3.png" alt="Comparison between SlowFast and SlowOnly" /></span>
							</li>
						</ol>
						<header>
						<h3>Download</h3>
						<p>WILL BE UPDATED LATER</p>
						</header>
					</div>